Deep Learning

我们既然用全连接的DNN已经取得非常好的识别效果，为啥不用DNN呢？--因为DNN没有考虑图片的空间结构。
it's strange to use networks with fully-connected layers to classify images. 
The reason is that such a network architecture does not take into account the spatial structure of the images

CNN有三个基本思想：局部可视野，共享权值，池化
Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling.

一次全卷积生成的一个隐层，相当于在输入图像中检测同一个特征。
This means that all the neurons in the first hidden layer detect exactly the same feature, just at different locations in the input image

CNN具有很好的图片变换不变性。
convolutional networks are well adapted to the translation invariance of images: 
move a picture of a cat (say) a little ways, and it's still an image of a cat

feature map：输入层到隐层的map，定义这个feature map的权值为共享权值，类似共享bias
For this reason, we sometimes call the map from the input layer to the hidden layer a feature map
the weights defining the feature map the shared weights
the bias defining the feature map in this way the shared bias

共享权值和bias通常称为定义一个核或过滤器
The shared weights and bias are often said to define a kernel or filter


Pooling layer通常卷积层之后立即接pooling层，来简化卷积层的输出
Pooling layers are usually used immediately after convolutional layers. 
What the pooling layers do is simplify the information in the output from the convolutional layer.

max-pooling：取最大
可以这样理解：在图片的任意区域中寻找一个给定特征是否存在。之后丢弃精确的位置信息。动机是一旦找到特征，其精确的位置信息，
相对于对其他特征的大致位置而言，不是太重要。这样大大降低后续层的参数数目。
We can think of max-pooling as a way for the network to ask whether a given feature is found anywhere in a region of the image. 
It then throws away the exact positional information. The intuition is that once a feature has been found, 
its exact location isn't as important as its rough location relative to other features. 
A big benefit is that there are many fewer pooled features, and so this helps reduce the number of parameters needed in later layers

L2 pooling：取每个值平方和开根号

提高结果的方法：
1，更好的激活函数，sigmoid->ReLU
2，扩展数据集。平移，旋转，拉伸。“elastic distortion”
3，Dropout。书中只在全连接层做dropout，conv层能不能做？没明白。
4，ensembling。跑几个模型，选最好的一个 

解释了为什么Dropout只在全连接层上做的原因：没有必要。因为conv层已经减弱了过拟合的风险。如权值共享，更好的正则化dropout
there's no need: the convolutional layers have considerable inbuilt resistance to overfitting. 
The reason is that the shared weights mean that convolutional filters are forced to learn from across the entire image. 
This makes them less likely to pick up on local idiosyncracies in the training data. 
And so there is less need to apply other regularizers, such as dropout.

我们解决了前面提到的DNN难以训练的梯度不稳定问题了？
答案是：当然没有。但通过几个方法来克服影响：
(1) Using convolutional layers greatly reduces the number of parameters in those layers, making the learning problem much easier; 
(2) Using more powerful regularization techniques (notably dropout and convolutional layers) to reduce overfitting, 
(3) Using rectified linear units instead of sigmoid neurons, to speed up training - empirically, often by a factor of 3-5; 
(4) Using GPUs and being willing to train for a long period of time

关于DNN是不是层数越多越好，越多才越是深度学习，作者认为不是。层数不是关心的重点，而宁愿把DNN当做一个工具，如更好的分类器
But beyond that, the number of layers is not of primary fundamental interest. 
Rather, the use of deeper networks is a tool to use to help achieve other goals - like better classification accuracies.

得到一个工作很好的DNN不是一件容易的事，必然伴随试错，需要进行多次的实验。
Getting a good, working network can involve a lot of trial and error, and occasional frustration. 
In practice, you should expect to engage in quite a bit of experimentation
