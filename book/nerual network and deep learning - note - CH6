Deep Learning

我们既然用全连接的DNN已经取得非常好的识别效果，为啥不用DNN呢？--因为DNN没有考虑图片的空间结构。
it's strange to use networks with fully-connected layers to classify images. 
The reason is that such a network architecture does not take into account the spatial structure of the images

CNN有三个基本思想：局部可视野，共享权值，池化
Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling.

一次全卷积生成的一个隐层，相当于在输入图像中检测同一个特征。
This means that all the neurons in the first hidden layer detect exactly the same feature, just at different locations in the input image

CNN具有很好的图片变换不变性。
convolutional networks are well adapted to the translation invariance of images: 
move a picture of a cat (say) a little ways, and it's still an image of a cat

feature map：输入层到隐层的map，定义这个feature map的权值为共享权值，类似共享bias
For this reason, we sometimes call the map from the input layer to the hidden layer a feature map
the weights defining the feature map the shared weights
the bias defining the feature map in this way the shared bias

共享权值和bias通常称为定义一个核或过滤器
The shared weights and bias are often said to define a kernel or filter


Pooling layer通常卷积层之后立即接pooling层，来简化卷积层的输出
Pooling layers are usually used immediately after convolutional layers. 
What the pooling layers do is simplify the information in the output from the convolutional layer.

max-pooling：取最大
可以这样理解：在图片的任意区域中寻找一个给定特征是否存在。之后丢弃精确的位置信息。动机是一旦找到特征，其精确的位置信息，
相对于对其他特征的大致位置而言，不是太重要。这样大大降低后续层的参数数目。
We can think of max-pooling as a way for the network to ask whether a given feature is found anywhere in a region of the image. 
It then throws away the exact positional information. The intuition is that once a feature has been found, 
its exact location isn't as important as its rough location relative to other features. 
A big benefit is that there are many fewer pooled features, and so this helps reduce the number of parameters needed in later layers

L2 pooling：取每个值平方和开根号

提高结果的方法：
1，更好的激活函数，sigmoid->ReLU
2，扩展数据集。平移，旋转，拉伸。“elastic distortion”
3，Dropout。书中只在全连接层做dropout，conv层能不能做？没明白。
4，ensembling。跑几个模型，选最好的一个 

解释了为什么Dropout只在全连接层上做的原因：没有必要。因为conv层已经减弱了过拟合的风险。如权值共享，更好的正则化dropout
there's no need: the convolutional layers have considerable inbuilt resistance to overfitting. 
The reason is that the shared weights mean that convolutional filters are forced to learn from across the entire image. 
This makes them less likely to pick up on local idiosyncracies in the training data. 
And so there is less need to apply other regularizers, such as dropout.

我们解决了前面提到的DNN难以训练的梯度不稳定问题了？
答案是：当然没有。但通过几个方法来克服影响：
(1) Using convolutional layers greatly reduces the number of parameters in those layers, making the learning problem much easier; 
(2) Using more powerful regularization techniques (notably dropout and convolutional layers) to reduce overfitting, 
(3) Using rectified linear units instead of sigmoid neurons, to speed up training - empirically, often by a factor of 3-5; 
(4) Using GPUs and being willing to train for a long period of time

关于DNN是不是层数越多越好，越多才越是深度学习，作者认为不是。层数不是关心的重点，而宁愿把DNN当做一个工具，如更好的分类器
But beyond that, the number of layers is not of primary fundamental interest. 
Rather, the use of deeper networks is a tool to use to help achieve other goals - like better classification accuracies.

得到一个工作很好的DNN不是一件容易的事，必然伴随试错，需要进行多次的实验。
Getting a good, working network can involve a lot of trial and error, and occasional frustration. 
In practice, you should expect to engage in quite a bit of experimentation

ALEXLET：
先把图片的短边缩放到256。在缩放后的图片上裁剪256*256区域，再从这个区域上随机抽取224×224图片。
 rescaling each image so the shorter side had length 256. 
 They then cropped out a 256×256 area in the center of the rescaled image. 
 Finally, KSH extracted random 224×224 subimages (and horizontal reflections) from the 256×256 images. 
 They did this random cropping as a way of expanding the training data, and thus reducing overfitting.

问题：
给图片一个很小的扰动，导致错分类。adversarial问题。原因没有搞清楚。
庆幸的是，实际应用下，这种扰动出现的概率非常低，以至于一般不影响网络的泛化性。

这个问题到现在才发现，也在另一方面说明了我们对神经网络理解的粗浅。

虽然在测试集上取得很大成功，但并不表明就万事大吉了。不认识和解决掉NN的一些基础性问题，就很难说我们攻克了识别问题。
But while this is encouraging it's not enough just to see improvements on benchmarks, or even real-world applications
There are fundamental phenomena which we still understand poorly, such as the existence of adversarial images

其他类型的NN：
RNN：隐层单元的输出不仅仅有前一层隐层的输入决定，可能与过去状态或其他输入相关。
RNNs are neural networks in which there is some notion of dynamic change over time.

Long short-term memory units (LSTMs): 
帮助RNN解决长时间依赖导致的梯度不稳定问题。使得RNN容易训练

DBF：
具有两个极好的特性：1，是一种泛化模型，2，是无监督学习模型。
那为啥现在DBF没有DNN/RNN红火不得宠呢？作者认为跟现在研究的风气有关，DNN结果很好，大家一窝蜂都去搞DNN去了，把DBN冷落了。
作者呼吁人们应该多投入写精力给DBN
My personal opinion is that DBNs and other generative models likely deserve more attention than they are currently receiving.

NN与其他领域的交叉
CNN+深度增强学习

====================================
DNN的未来
====================================
技术的发展起起伏伏，三年河东三年河西。很难讲NN会怎么样，谁也不能否认什么时候会出现一个黑科技完胜DNN

作者的一个预测：深度学习的位置：能够学习层次化概念，建立多级抽象，表达现实世界。但也并不意味着未来的DNN就不会有大的改变。
I will make one prediction: I believe deep learning is here to stay. The ability to learn hierarchies of concepts, 
building up multiple layers of abstraction, seems to be fundamental to making sense of the world. 
This doesn't mean tomorrow's deep learners won't be radically different than today's.

DNN和AI：一切断言都为时过早，一切难以料定。但DNN确实创造了很多机会。

